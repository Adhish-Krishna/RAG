{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing output/text/anatomy_vol_1.txt...\n",
      "Processing output/text/anatomy_vol_2.txt...\n",
      "Processing output/text/anatomy_vol_3.txt...\n",
      "Semantic chunking completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy's large English model for better performance on semantic chunking\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Increase the maximum text length\n",
    "nlp.max_length = 2000000  # Set this to a larger value as needed\n",
    "\n",
    "# Define paths\n",
    "text_directory = 'output/text/'\n",
    "chunk_output_folder = 'output/chunks/'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(chunk_output_folder, exist_ok=True)\n",
    "\n",
    "def semantic_chunking(text, min_chunk_size=50, max_chunk_size=200):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    word_count = 0\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        chunk.append(sent.text)\n",
    "        word_count += len(sent)\n",
    "\n",
    "        # If the current chunk exceeds the maximum chunk size, finalize the chunk\n",
    "        if word_count >= max_chunk_size:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "            chunk = []\n",
    "            word_count = 0\n",
    "        \n",
    "        # If the chunk is within the minimum size and the next sentence would push it over the max size, finalize the chunk\n",
    "        elif word_count >= min_chunk_size and (word_count + len(sent)) > max_chunk_size:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "            chunk = []\n",
    "            word_count = 0\n",
    "    \n",
    "    # Add any remaining text as the last chunk\n",
    "    if chunk:\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_text_file(text_file_path, min_chunk_size, max_chunk_size):\n",
    "    with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    chunks = semantic_chunking(text, min_chunk_size, max_chunk_size)\n",
    "\n",
    "    base_filename = os.path.splitext(os.path.basename(text_file_path))[0]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_filename = os.path.join(chunk_output_folder, f\"{base_filename}_chunk_{i+1}.txt\")\n",
    "        with open(chunk_filename, 'w', encoding='utf-8') as chunk_file:\n",
    "            chunk_file.write(chunk)\n",
    "\n",
    "def chunk_all_text_files(min_chunk_size=50, max_chunk_size=200):\n",
    "    text_files = [f for f in os.listdir(text_directory) if f.lower().endswith('.txt')]\n",
    "\n",
    "    for text_file in text_files:\n",
    "        text_file_path = os.path.join(text_directory, text_file)\n",
    "        print(f\"Processing {text_file_path}...\")\n",
    "        process_text_file(text_file_path, min_chunk_size, max_chunk_size)\n",
    "    print(\"Semantic chunking completed.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    chunk_all_text_files(min_chunk_size=50, max_chunk_size=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SURYA\\anaconda3\\envs\\RAG\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\SURYA\\anaconda3\\envs\\RAG\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fea065fce8c4a419a5a74b6ffa47ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [[1.0200987 1.1858795 1.2546614 1.2967848 1.3206853]]\n",
      "Indices: [[6057 2658 2674 1394 3002]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def chunk_text_with_textsplit(text, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Chunk text into smaller pieces.\n",
    "    \"\"\"\n",
    "    # Simple chunking by length\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def generate_embeddings(text_chunks):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text chunks.\n",
    "    \"\"\"\n",
    "    embeddings = model.encode(text_chunks, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Create a FAISS index and add the embeddings.\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # Using L2 distance\n",
    "\n",
    "    # Add embeddings to the index\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def search_faiss_index(index, query_embedding, k=5):\n",
    "    \"\"\"\n",
    "    Search the FAISS index for the top-k most similar vectors.\n",
    "    \"\"\"\n",
    "    distances, indices = index.search(np.array([query_embedding]), k)\n",
    "    return distances, indices\n",
    "\n",
    "def process_and_store_embeddings(text_directory):\n",
    "    text_files = [f for f in os.listdir(text_directory) if f.lower().endswith('.txt')]\n",
    "    \n",
    "    all_chunks = []\n",
    "    all_embeddings = []\n",
    "\n",
    "    for text_file in text_files:\n",
    "        text_file_path = os.path.join(text_directory, text_file)\n",
    "        with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        chunks = chunk_text_with_textsplit(text, chunk_size=1000)\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = generate_embeddings(all_chunks)\n",
    "\n",
    "    # Create and store embeddings in FAISS\n",
    "    index = create_faiss_index(embeddings)\n",
    "    \n",
    "    # Save the index to disk\n",
    "    faiss.write_index(index, 'faiss_index.index')\n",
    "\n",
    "def search(text, index_path='faiss_index.index'):\n",
    "    # Load the FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Generate embedding for the query text\n",
    "    query_embedding = model.encode([text])[0]\n",
    "\n",
    "    # Perform the search\n",
    "    distances, indices = search_faiss_index(index, query_embedding, k=5)\n",
    "    return distances, indices\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define your text directory\n",
    "    text_directory = 'output/chunks/'\n",
    "\n",
    "    # Process and store embeddings\n",
    "    process_and_store_embeddings(text_directory)\n",
    "\n",
    "    # Example search\n",
    "    query = \"context of the book\"\n",
    "    distances, indices = search(query)\n",
    "    print(f\"Distances: {distances}\")\n",
    "    print(f\"Indices: {indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a22621a7af74f4988cb6608582ce78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m distances, indices \u001b[38;5;241m=\u001b[39m search(query)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Retrieve documents based on search results\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m retrieved_docs \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m retrieved_docs:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(doc)\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mretrieve_documents\u001b[1;34m(text_directory, indices)\u001b[0m\n\u001b[0;32m      5\u001b[0m document_chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m indices[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mall_chunks\u001b[49m):\n\u001b[0;32m      8\u001b[0m         document_chunks\u001b[38;5;241m.\u001b[39mappend(all_chunks[index])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m document_chunks\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "def retrieve_documents(text_directory, indices):\n",
    "    \"\"\"\n",
    "    Retrieve the documents corresponding to the indices returned by the FAISS search.\n",
    "    \"\"\"\n",
    "    document_chunks = []\n",
    "    for index in indices[0]:\n",
    "        if index >= 0 and index < len(all_chunks):\n",
    "            document_chunks.append(all_chunks[index])\n",
    "    \n",
    "    return document_chunks\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define your text directory\n",
    "    text_directory = 'output/chunks/'\n",
    "\n",
    "    # Process and store embeddings\n",
    "    process_and_store_embeddings(text_directory)\n",
    "\n",
    "    # Example search\n",
    "    query = \"example query text\"\n",
    "    distances, indices = search(query)\n",
    "\n",
    "    # Retrieve documents based on search results\n",
    "    retrieved_docs = retrieve_documents(text_directory, indices)\n",
    "    for doc in retrieved_docs:\n",
    "        print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oollama import Mistral\n",
    "\n",
    "# Update with the correct model path\n",
    "model_path = r'C:\\\\Users\\SURYA\\\\.ollama\\\\models\\\\manifests\\\\gistry.ollama.ai\\\\library\\\\mistral'\n",
    "\n",
    "# Initialize Mistral with the local model path\n",
    "mistral = Mistral(model_path=model_path)\n",
    "\n",
    "def format_output(text):\n",
    "    \"\"\"\n",
    "    Use the local Mistral model to format the output text.\n",
    "    \"\"\"\n",
    "    formatted_text = mistral.format(text)\n",
    "    return formatted_text\n",
    "\n",
    "def search_faiss(query):\n",
    "    \"\"\"\n",
    "    Dummy implementation of FAISS search.\n",
    "    Replace this with your actual FAISS search code.\n",
    "    \"\"\"\n",
    "    # Example placeholder for FAISS search\n",
    "    return \"Sample results from FAISS search related to: \" + query\n",
    "\n",
    "def retrieve_and_format_results(query):\n",
    "    \"\"\"\n",
    "    Retrieve and format results using Mistral.\n",
    "    \"\"\"\n",
    "    # Retrieve results from FAISS\n",
    "    results = search_faiss(query)  # or however you retrieve results\n",
    "\n",
    "    # Format results using the local Mistral model\n",
    "    formatted_results = format_output(results)\n",
    "    return formatted_results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    query = \"Describe the anatomy of the human heart.\"\n",
    "    formatted_results = retrieve_and_format_results(query)\n",
    "    print(formatted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load the embedding model\n",
    "model = SentenceTransformer('sentence-transformers/scibert-scivocab-cased')\n",
    "\n",
    "def get_embeddings(chunks):\n",
    "    return [model.encode(chunk) for chunk in chunks]\n",
    "\n",
    "def store_embeddings(embeddings):\n",
    "    dimension = embeddings[0].shape[0]  # Size of the embedding vector\n",
    "    index = faiss.IndexFlatL2(dimension)  # Initialize a FAISS index\n",
    "    index.add(np.array(embeddings))  # Add embeddings to the index\n",
    "    return index\n",
    "\n",
    "def save_faiss_index(index, path):\n",
    "    faiss.write_index(index, path)\n",
    "\n",
    "def load_faiss_index(path):\n",
    "    return faiss.read_index(path)\n",
    "\n",
    "def search_faiss(query, index, model, k=5):\n",
    "    query_embedding = model.encode([query])\n",
    "    D, I = index.search(np.array(query_embedding), k)  # Search for top-k similar embeddings\n",
    "    return I  # Returns the indices of the most similar chunks\n",
    "\n",
    "# Example chunks of text\n",
    "chunks = [\"Chunk 1 text here\", \"Chunk 2 text here\", \"Chunk 3 text here\"]\n",
    "\n",
    "# Embed and store embeddings\n",
    "embeddings = get_embeddings(chunks)\n",
    "faiss_index = store_embeddings(embeddings)\n",
    "\n",
    "# Save and load FAISS index\n",
    "save_faiss_index(faiss_index, 'path/to/faiss_index.index')\n",
    "faiss_index = load_faiss_index('path/to/faiss_index.index')\n",
    "\n",
    "# Perform a search\n",
    "query = \"Describe the anatomy of the human heart.\"\n",
    "indices = search_faiss(query, faiss_index, model, k=5)\n",
    "\n",
    "# Retrieve and print results\n",
    "retrieved_chunks = [chunks[i] for i in indices[0]]\n",
    "print(\"Retrieved Chunks:\")\n",
    "for chunk in retrieved_chunks:\n",
    "    print(chunk)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
