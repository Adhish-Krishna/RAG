{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SentenceTransformersTokenTextSplitter in module langchain_text_splitters.sentence_transformers:\n",
      "\n",
      "class SentenceTransformersTokenTextSplitter(langchain_text_splitters.base.TextSplitter)\n",
      " |  SentenceTransformersTokenTextSplitter(chunk_overlap: 'int' = 50, model_name: 'str' = 'sentence-transformers/all-mpnet-base-v2', tokens_per_chunk: 'Optional[int]' = None, **kwargs: 'Any') -> 'None'\n",
      " |  \n",
      " |  Splitting text to tokens using sentence model tokenizer.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SentenceTransformersTokenTextSplitter\n",
      " |      langchain_text_splitters.base.TextSplitter\n",
      " |      langchain_core.documents.transformers.BaseDocumentTransformer\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, chunk_overlap: 'int' = 50, model_name: 'str' = 'sentence-transformers/all-mpnet-base-v2', tokens_per_chunk: 'Optional[int]' = None, **kwargs: 'Any') -> 'None'\n",
      " |      Create a new TextSplitter.\n",
      " |  \n",
      " |  count_tokens(self, *, text: 'str') -> 'int'\n",
      " |  \n",
      " |  split_text(self, text: 'str') -> 'List[str]'\n",
      " |      Split text into multiple components.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_max_length_equal_32_bit_integer': 'int'}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_text_splitters.base.TextSplitter:\n",
      " |  \n",
      " |  create_documents(self, texts: 'List[str]', metadatas: 'Optional[List[dict]]' = None) -> 'List[Document]'\n",
      " |      Create documents from a list of texts.\n",
      " |  \n",
      " |  split_documents(self, documents: 'Iterable[Document]') -> 'List[Document]'\n",
      " |      Split documents.\n",
      " |  \n",
      " |  transform_documents(self, documents: 'Sequence[Document]', **kwargs: 'Any') -> 'Sequence[Document]'\n",
      " |      Transform sequence of documents by splitting them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_text_splitters.base.TextSplitter:\n",
      " |  \n",
      " |  from_huggingface_tokenizer(tokenizer: 'Any', **kwargs: 'Any') -> 'TextSplitter' from abc.ABCMeta\n",
      " |      Text splitter that uses HuggingFace tokenizer to count length.\n",
      " |  \n",
      " |  from_tiktoken_encoder(encoding_name: 'str' = 'gpt2', model_name: 'Optional[str]' = None, allowed_special: \"Union[Literal['all'], AbstractSet[str]]\" = set(), disallowed_special: \"Union[Literal['all'], Collection[str]]\" = 'all', **kwargs: 'Any') -> 'TS' from abc.ABCMeta\n",
      " |      Text splitter that uses tiktoken encoder to count length.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.documents.transformers.BaseDocumentTransformer:\n",
      " |  \n",
      " |  async atransform_documents(self, documents: 'Sequence[Document]', **kwargs: 'Any') -> 'Sequence[Document]'\n",
      " |      Asynchronously transform a list of documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          documents: A sequence of Documents to be transformed.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A sequence of transformed Documents.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.documents.transformers.BaseDocumentTransformer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "help(SentenceTransformersTokenTextSplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
