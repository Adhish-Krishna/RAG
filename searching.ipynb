{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SURYA\\anaconda3\\envs\\RAG\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\SURYA\\anaconda3\\envs\\RAG\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't have the specific question from the context provided. Could you please provide the exact question for me to answer?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SURYA\\anaconda3\\envs\\RAG\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Paths\n",
    "MODEL = \"mistral\"  # Use the Mistral model\n",
    "model = Ollama(model=MODEL)\n",
    "\n",
    "text_chunks_folder = 'output/chunks/'\n",
    "faiss_index_path = 'model_embeddings/faiss_index.index'\n",
    "vector_db_path = 'model_embeddings/embeddings.pkl'\n",
    "\n",
    "def load_text_chunks_from_folder(folder_path):\n",
    "    text_chunks = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text_chunks.append(file.read())\n",
    "    return text_chunks\n",
    "\n",
    "def embed_text_chunks(text_chunks, model):\n",
    "    return model.encode(text_chunks)\n",
    "\n",
    "def save_embeddings(embeddings, text_chunks, path):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump((text_chunks, embeddings), file)\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "def build_faiss_index(embeddings):\n",
    "    dim = embeddings.shape[1]  # Dimension of embeddings\n",
    "    index = faiss.IndexFlatL2(dim)  # L2 distance index\n",
    "    index.add(embeddings)  # Add embeddings to index\n",
    "    return index\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def format_output(text):\n",
    "    from langchain_community.llms import Ollama\n",
    "\n",
    "\n",
    "def format_output(context, question):\n",
    "    \"\"\"\n",
    "    Use Mistral model to generate formatted output.\n",
    "    \"\"\"\n",
    "    # Define the template\n",
    "    template = \"\"\"\n",
    "    Answer the question based on the context below. If you can't \n",
    "    answer the question, reply \"I don't know\".\n",
    "    Only give me the answers based on the context below.\n",
    "    Only answer the question asked. Do not provide additional information.\n",
    "    Give a clear and concise answer.\n",
    "    \n",
    "\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the template with context and question\n",
    "    prompt_text = template.format(context=context, question=question)\n",
    "\n",
    "    # Generate a response using the Mistral model\n",
    "    response = model(prompt_text)\n",
    "    # Return the formatted output\n",
    "    return response\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def search_faiss(query, index, model, k=5):\n",
    "    query_embedding = model.encode([query])\n",
    "    D, I = index.search(np.array(query_embedding), k)  # Search for top-k similar embeddings\n",
    "    return I[0]  # Returns the indices of the most similar chunks\n",
    "\n",
    "def retrieve_and_format_results(query, index, text_chunks, model):\n",
    "    indices = search_faiss(query, index, model)\n",
    "    \n",
    "    # Handle case where no indices are returned\n",
    "    if not indices.size:\n",
    "        return \"No relevant information found.\"\n",
    "\n",
    "    # Check for valid indices\n",
    "    valid_indices = [i for i in indices if 0 <= i < len(text_chunks)]\n",
    "    results = \" \".join([text_chunks[i] for i in valid_indices])  # Concatenate retrieved chunks\n",
    "    \n",
    "    formatted_results = format_output(results ,query)\n",
    "    return formatted_results\n",
    "\n",
    "# Initialize models\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Load or create embeddings and FAISS index\n",
    "if os.path.exists(vector_db_path):\n",
    "    text_chunks, embeddings = load_embeddings(vector_db_path)\n",
    "else:\n",
    "    text_chunks = load_text_chunks_from_folder(text_chunks_folder)\n",
    "    embeddings = embed_text_chunks(text_chunks, embedding_model)\n",
    "    save_embeddings(embeddings, text_chunks, vector_db_path)\n",
    "\n",
    "faiss_index = build_faiss_index(embeddings)\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG :(\n",
      "\n",
      " DNA (Deoxyribonucleic Acid) is a long, twisted molecule composed primarily of two strands that are complementary to each other. Each side rail of the DNA ladder is made up of alternating sugar and phosphate groups. The sugars are deoxyribose sugars, hence the name Deoxyribonucleic Acid.\n",
      "\n",
      "   Flowchart:\n",
      "   1. Sugar: Deoxyribose (2-deoxypentose)\n",
      "       - A 5-carbon sugar with a hydroxyl group at the second carbon and a hydrogen atom at the third carbon, instead of the usual ketone group that would be found in other pentose sugars.\n",
      "   2. Phosphate:\n",
      "       - A negatively charged molecule consisting of one phosphorus atom bonded to three oxygen atoms (a trioxophosphate).\n",
      "   3. Bases: There are four types of nitrogenous bases in DNA: adenine (A), thymine (T), guanine (G), and cytosine (C)\n",
      "       - These bases pair up specifically during the formation of the double helix structure, with A pairing with T, and C pairing with G.\n",
      "   4. Phosphodiester bond: The nucleotides are linked together by phosphodiester bonds between their sugars and phosphate groups to form the sugar-phosphate backbone of the DNA strand.\n",
      "   5. Base pairs: The two strands of the DNA molecule are held together by hydrogen bonds between the base pairs (A-T and C-G). This forms a double-stranded helix structure, with the bases on the inside of the helix and the sugar-phosphate backbones on the outside.\n",
      "   6. The nucleus: The DNA is housed within the nucleus of the cell, where it is loosely contained as chromatin until the cell is ready to divide. When a cell divides, the chromatin condenses into chromosomes, which are composed of DNA and proteins.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"what are DNA made up of, explain in detail with help of flowchart\" \n",
    "formatted_results = retrieve_and_format_results(query, faiss_index, text_chunks, embedding_model)\n",
    "print(\"RAG :(\\n\")\n",
    "print(formatted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
